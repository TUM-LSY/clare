<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion - Römer, Ralf; Schoellig, Angela P.">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="This paper presents a novel approach to safe deployment of diffusion policies without compromising task success.">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="robotics, imitation learning, VLAs, diffusion policies, continual learning">
  <!-- TODO: List all authors -->
  <meta name="author" content="Römer, Ralf; Zhang, Yi; Schoellig, Angela P.">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="Learning Systems and Robotics Lab (LSY), TUM">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="This paper presents a novel approach to safe deployment of diffusion policies without compromising task success.">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://tum-lsy.github.io/clare/">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://tum-lsy.github.io/clare/static/images/social_preview.png">
  <meta property="og:image:width" content="1500">
  <meta property="og:image:height" content="804">
  <meta property="og:image:alt" content="CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion">
  <meta property="article:published_time" content="2025-10-10T00:00:00.000Z">
  <meta property="article:author" content="Römer, Ralf; Zhang, Yi; Schoellig, Angela P.">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="robotics">
  <meta property="article:tag" content="imitation learning">
  <meta property="article:tag" content="VLAs">
  <meta property="article:tag" content="diffusion policies">
  <meta property="article:tag" content="continual learning">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion">
  <meta name="citation_author" content="Römer, Ralf">
  <meta name="citation_author" content="Zhang, Yi">
  <meta name="citation_author" content="Schoellig, Angela P.">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="todo">
  <meta name="citation_pdf_url" content="https://arxiv.org/pdf/todo">
  
  <!-- Additional SEO, og #2563eb -->
  <meta name="theme-color" content="#3070B3">
  <meta name="msapplication-TileColor" content="#3070B3">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion</title>

  <!-- Favicon and App Icons -->
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico"> -->
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Custom color overrides -->
  <style>
    section.hero.is-light {
      background-color: #efffef !important;
    }
  </style>
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion",
    "description": "This paper presents a novel approach to safe deployment of diffusion policies without compromising task success.",
    "author": [
      {
        "@type": "Person",
        "name": "Ralf Römer",
        "affiliation": {
          "@type": "Organization",
          "name": "Learning Systems and Robotics Lab, Technical University of Munich"
        }
      },
      {
        "@type": "Person",
        "name": "Yi Zhang",
        "affiliation": {
          "@type": "Organization",
          "name": "Learning Systems and Robotics Lab, Technical University of Munich"
        }
      },
      {
        "@type": "Person",
        "name": "Angela P. Schoellig",
        "affiliation": {
          "@type": "Organization",
          "name": "Learning Systems and Robotics Lab, Technical University of Munich"
        }
      },

    ],
    "datePublished": "2025-10-10",
    "publisher": {
      "@type": "Organization",
      "name": "Neural Information Processing Systems Foundation",
      "url": "https://neurips.cc"
    },
    "url": "https://tum-lsy.github.io/clare/",
    "image": "https://tum-lsy.github.io/clare/static/images/social_preview.png",
    "keywords": ["robotics", "imitation learning", "VLAs", "diffusion policies", "continual learning"],
    "abstract": "To teach robots complex manipulation tasks, it is now a common practice to fine-tune a pre-trained vision-language-action model (VLA) on task-specific data. However, since this recipe updates existing representations, it is unsuitable for long-term operation in the real world, where robots must continually adapt to new tasks and environments while retaining the knowledge they have already acquired. Existing continual learning methods for robotics commonly require storing previous data (exemplars), struggle with long task sequences, or rely on task identifiers for deployment. To address these limitations, we propose CLARE, a general, parameter-efficient framework for non-exemplar continual learning with VLAs. CLARE introduces lightweight modular adapters into selected feedforward layers and autonomously expands the model only where necessary when learning a new task, guided by layer-wise feature similarity. During deployment, an autoencoder-based routing mechanism dynamically activates the most relevant adapters without requiring task labels. Through extensive experiments on the LIBERO benchmark, we show that CLARE achieves high performance on new tasks without catastrophic forgetting of earlier tasks, significantly outperforming even exemplar-based methods.",
    "citation": "@article{clare2025,\n  title={CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion},\n  author={Ralf R\"omer and Yi Zhang and Angela P. Schoellig},\n  booktitle={TODO},\n  year={TODO}\n}",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://tum-lsy.github.io/clare/"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Robotics"
      },
      {
        "@type": "Thing", 
        "name": "Imitation Learning"
      },
      {
        "@type": "Thing",
        "name": "VLAs"
      },
      {
        "@type": "Thing",
        "name": "Diffusion Policies"
      },
      {
        "@type": "Thing",
        "name": "Continual Learning"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "Learning Systems and Robotics Lab, Technical University of Munich",
    "url": "https://www.dynsyslab.org",
    "logo": "https://tum-lsy.github.io/clare/static/images/learnsyslab_logo.ico",
    "sameAs": [
      "https://www.linkedin.com/company/learnsyslab",
      "https://github.com/utiasDSL"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>


  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-2 publication-title">CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion</h1>

            <!-- TODO: Replace with your paper venue and year -->
            <!-- <div class="is-flex is-justify-content-center is-align-items-center" style="margin-bottom: 1em;">
              <img src="static/images/NeurIPS-logo.svg" alt="NeurIPS Logo" style="height:5em; margin-right:0.5em;">
              <span class="is-size-4 has-text-weight-bold" style="vertical-align:middle; color:#68448B;">NeurIPS 2025</span>
            </div> -->

            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                <a href="https://ralfroemer99.github.io" target="_blank">Ralf Römer</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/yi-zhang-01a8aa245/" target="_blank">Yi Zhang</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="https://www.dynsyslab.org/prof-angela-schoellig/" target="_blank">Angela P. Schoellig</a>,</span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block">Learning Systems and Robotics Lab, Technical University of Munich</span>
                <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution</small></span>
              </div>

              <div class="is-flex is-justify-content-center is-align-items-center" style="gap: 0.5em; margin: 1em 0;">
                <!-- <img src="static/images/learnsyslab_logo.png" alt="LSY" style="height:4em;"> -->
                <img src="static/images/Logo_of_the_Technical_University_of_Munich.svg" alt="TUM" style="height:3em;">
              </div>

              <div class="column has-text-centered">
                <div class="publication-links" style="display: flex; flex-wrap: wrap; justify-content: center; gap: 0.5em;">
                  <span class="link-block" style="width:auto;">
                    <a href="https://arxiv.org/pdf/todo" target="_blank"
                    class="external-link button is-normal is-rounded is-dark pub-btn">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>PDF</span>
                    </a>
                  </span>
                  <span class="link-block" style="width:auto;">
                    <a href="https://arxiv.org/abs/todo" target="_blank"
                    class="external-link button is-normal is-rounded is-dark pub-btn">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv (soon)</span>
                    </a>
                  </span>
                  <span class="link-block" style="width:auto;">
                    <a href="https://anonymous.4open.science/r/clare-ral/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark pub-btn">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                </div>
              </div>
              <style>
              @media (max-width: 600px) {
                .publication-links {
                  flex-direction: column !important;
                  align-items: center !important;
                }
                .publication-links .link-block {
                  width: 100% !important;
                  display: flex;
                  justify-content: center;
                  margin-bottom: 0.5em;
                }
                .publication-links .pub-btn {
                  width: 100% !important;
                  justify-content: center;
                }
              }
              </style>
              </style>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<div class="columns is-centered is-vcentered" style="align-items: flex-start; margin-top: 1em;">
          <figure style="display: flex; flex-direction: column; align-items: center;">
            <img src="static/images/overview.png" alt="Motivation" style="width:30%; object-fit:contain;">
            <!-- <figcaption style="margin-top:0.5em;">Figure 1: </figcaption> -->
          </figure>
        </div>

<!-- Paper abstract -->
<section class="section hero is-light" id="examples">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
            To teach robots complex manipulation tasks, it is now a common practice to fine-tune a pre-trained vision-language-action model (VLA) 
            on task-specific data. However, since this recipe updates existing representations, it is unsuitable
            for long-term operation in the real world, where robots must <strong>continually adapt</strong> to new tasks and environments while 
            <strong>retaining the knowledge they have already acquired</strong>. Existing continual learning methods for robotics commonly require storing
            previous data (exemplars), struggle with long task sequences,
            or rely on task identifiers for deployment. To address these
            limitations, we propose CLARE, a <strong>general, parameter-efficient</strong>
            framework for non-exemplar continual learning with VLAs.
            CLARE introduces <strong>lightweight modular adapters</strong> into selected
            feedforward layers and autonomously expands the model only
            where necessary when learning a new task, guided by layer-wise
            feature similarity. During deployment, an autoencoder-based
            routing mechanism dynamically activates the most relevant
            adapters <strong>without requiring task labels</strong>. Through extensive
            experiments on the LIBERO benchmark, we show that CLARE
            achieves <strong>high performance on new tasks without catastrophic
            forgetting</strong> of earlier tasks, significantly outperforming even
            exemplar-based methods.
          </p>
        </div>
      </div>
    </div>
    <style>
    @media (max-width: 600px), (max-width: 900px) and (orientation: landscape) {
      .robot-pretzel-img {
      justify-content: center !important;
      margin: 0 !important;
      }
    }
    </style>
  </div>
</section>
<!-- End paper abstract -->

<!-- Why -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Motivation</h2>
        
        <div class="content has-text-justified">
          <!-- Text -->
          <p>
            Deploying robots in dynamic real-world environments, such as homes or hospitals, requires them to continuously acquire new skills without losing previously learned capabilities. 
            Standard fine-tuning of VLAs often leads to catastrophic forgetting, where new training overwrites critical prior knowledge. 
            Moreover, reliance on storing past data for replay is often impractical due to privacy or storage constraints, and robots operating autonomously rarely have access to "oracle" task identifiers to tell them which skill to use. 
            We address these challenges with a framework designed for lifelong learning that relies neither on stored exemplars nor on external task labels.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Why -->

<!-- How -->
<section class="section hero is-light">
  <div class="container is-max-desktop has-text-centered">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        
        <div class="columns is-multiline is-centered" style="margin-top: 1em;">
          <div class="column is-half-tablet is-half-desktop">
            <figure style="display:flex;flex-direction:column;align-items:center;">
              <img src="static/images/expansion_routing.png" alt="Robomimic results" style="width:100%; max-width:600px; object-fit:contain;">
              <figcaption style="margin-top:0.5em;">Expansion and Routing mechanisms</figcaption>
            </figure>
          </div>

          <div class="column is-half-tablet is-half-desktop">
            <figure style="display:flex;flex-direction:column;align-items:center;">
              <img src="static/images/policy.png" alt="Real-world results" style="width:100%; max-width:600px; object-fit:contain;">
              <figcaption style="margin-top:0.5em;">DiT architecture.</figcaption>
            </figure>
          </div>
        <!-- <img src="static/images/overview.png" alt="Shield illustration" style="width:100%; height:auto; margin-bottom:2em;"> -->
        <div class="content has-text-justified">
          <!-- Text -->
          <p>
            We introduce CLARE (Continual Learning via Adapter Routing and Expansion), a framework that injects lightweight, trainable adapters into the feedforward layers of a frozen, pre-trained VLA. 
            CLARE utilizes a Dynamic Expansion strategy that monitors feature statistics; it only adds new parameters when the incoming task data is sufficiently novel, preventing unnecessary capacity increase. 
            During deployment, an Autonomous Routing mechanism uses autoencoder-based discriminators to analyze input features and dynamically activate the most relevant adapter for the current situation. 
            This allows the robot to seamlessly switch between skills without needing explicit task commands.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End How -->

<!-- Results -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <div class="columns is-centered is-vcentered" style="align-items: flex-start; margin-top: 1em;">
          <figure style="display: flex; flex-direction: column; align-items: center;">
            <img src="static/images/baseline_plot.png" alt="Sorting Paths" style="width:100%; object-fit:contain;">
          </figure>
        </div> 
        <div class="content has-text-justified">
          <!-- Text -->
          <p>
            We evaluated CLARE on the LIBERO benchmark using a sequence of 10 long-horizon manipulation tasks that require both language understanding and precise motor control. 
            Our experiments demonstrate that CLARE achieves high success rates on new tasks (Forward Transfer) while effectively preventing the degradation of performance on previously learned tasks (Negative Backward Transfer). 
            It significantly outperforms baselines like Sequential Fine-Tuning and LoRA, and even exceeds the performance of some methods that rely on experience replay, all while increasing the total parameter count by only approximately 2% per task.
          </p>
        </div>
        <div class="columns is-multiline is-centered" style="margin-top: 1em;">
          <div class="column is-half-tablet is-half-desktop">
            <figure style="display:flex;flex-direction:column;align-items:center;">
              <img src="static/images/baseline_table.png" alt="Robomimic results" style="width:100%; max-width:600px; object-fit:contain;">
              <figcaption style="margin-top:0.5em;">Baseline comparison. CLARE achieves the highest overall
      performance, as measured by AUC, and demonstrates strong capabilities
to acquire new skills without forgetting.</figcaption>
            </figure>
          </div>

          <div class="column is-half-tablet is-half-desktop">
            <figure style="display:flex;flex-direction:column;align-items:center;">
              <img src="static/images/ablation.png" alt="Real-world results" style="width:100%; max-width:600px; object-fit:contain;">
              <figcaption style="margin-top:0.5em;">Ablation study for the dynamic expansion threshold γ. Increasing γ
significantly reduces the number of adapters added to the model but slightly
reduces the capability to learn new tasks, as shown by the small decrease
in AUC and FWT. In contrast, NBT remains at around zero, indicating that
the model does not exhibit catastrophic forgetting.</figcaption>
            </figure>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>


<!-- Highlights -->
<section class="section  hero is-light">
  <div class="container is-max-desktop has-text-centered">
    <h2 class="title is-3">Highlights</h2>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          PACS
          <ul>
            <li><strong>Non-Exemplar Learning:</strong> Enables VLAs to learn sequentially without storing or replaying past data, respecting privacy and storage limits.</li>
            <li><strong>Autonomous Adapter Routing:</strong> Automatically selects the correct task-specific module during inference based on feature similarity, eliminating the need for task IDs.</li>
            <li><strong>Dynamic Expansion:</strong> Efficiently adds new parameters only when necessary, maintaining a sub-linear growth in model size.</li>
            <li><strong>Prevents Catastrophic Forgetting:</strong> Preserves pre-trained representations while learning new skills, ensuring long-term stability.</li>
            <li><strong>Superior Performance:</strong> Outperforms state-of-the-art continual learning baselines on the challenging LIBERO continual learning benchmark.</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Highlights -->

<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths content">
        <div class="bibtex-header">
          <h2 class="title">BibTeX</h2>
          <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
            <i class="fas fa-copy"></i>
            <span class="copy-text">Copy</span>
          </button>
        </div>
        <pre id="bibtex-code"><code>@article{clare2025,
          title={CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion},
          author={Ralf R{\"o}mer and Yi Zhang and Angela P. Schoellig},
          journal={arXiv preprint arXiv:TODO},
          year={2025}
        }</code></pre>
      </div>
    </div>
  </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths content">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            It is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
        <!-- Visitor Counter -->
        <div id="visitor-count"
             style="text-align:center; margin-top:-1.25em; margin-bottom:-2.25em; font-size:0.9em; color:#999; opacity:0; transition:opacity 0.3s;">
          Visitors: <span id="count">–</span>
        </div>
        <script>
(async () => {
  const container = document.getElementById("visitor-count");
  const span = document.getElementById("count");
  const url = "https://hitscounter.dev/api/hit?output=json&url=https%3A%2F%2Ftum-lsy.github.io%2Fclare%2F&label=Visitors&icon=arrow-right-circle-fill&color=%236ea8fe&message=&style=for-the-badge&tz=Europe%2FBerlin";
  try {
    const res = await fetch(url, { cache: "no-store", headers: { "Accept": "application/json" } });
    const data = await res.json();
    let val = null;
    if (data) {
      if (typeof data.total_hits === "number") val = data.total_hits;
      else if (typeof data.today_hits === "number") val = data.today_hits;
      else if (typeof data.count === "number") val = data.count;
    }
    if (typeof val === "number" && !Number.isNaN(val)) {
      span.textContent = val.toLocaleString();
      container.style.opacity = "1";
    } else {
      span.textContent = "n/a";
      container.style.opacity = "1";
    }
  } catch (e) {
    span.textContent = "error";
    container.style.opacity = "1";
    console.error("Visitor counter error:", e);
  }
})();
        </script>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
